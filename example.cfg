# Configuration file format
#
# [Input]
# train_location = Path to directory containing training data files. There must be a file for each featureset.
# test_location = Path to directory containing training data files. There must be a file for each featureset. If unspecified, cross-validation is performed.
# featuresets = List of prefixes for the files containing the features you would like to train/test on.
# classifiers = List of sklearn classifiers to try using. Acceptable values are 'logistic', 'svm_linear', 'svm_radial', 'naivebayes', 'dtree', 'rforest', and 'gradient'.
#               logistic = Logistic regression using LibLinear
#               svm_linear = SVM using LibLinear
#               svm_radial = SVM using LibSVM
#               naivebayes = Multinomial Naive Bayes
#               dtree = Decision Tree
#               rforest = Random Forest
#               gradient = Gradient Boosting
# fixed_parameters = List of dicts containing parameters you want to have fixed for each classifier in list. Any empty ones will be ignored (and the defaults will be used).
# suffix = The file format the training/test files are in. Valid option are '.tsv', '.megam', and '.jsonlines' (one complete JSON dict per line in the file).
#
# [Tuning]
# grid_search = Whether or not to perform grid search to find optimal parameters for classifier.
# objective = The objective function to use for tuning. Valid options are 'f1_score_micro' (micro-averaged f-score), 'f1_score_macro' (macro-averaged f-score), 'f1_score_least_frequent' (optimize the f1 score of the least frequent class), 'accuracy', 'spearman' (Spearman rank-correlation), 'pearson' (Pearson correlation), and 'kendall_tau' (Kendall's tau).
# param_grids = List of parameter grids to search for each classifier. Each parameter grid should be a list of of dictionaries mapping from strings to lists of parameter values. When you specify an empty list for a classifier, the default parameter grid for that classifier will be searched.
#
# [Output]
# probability = Whether or not to output probabilities for each class instead of the most probable class for each instance. Only really makes a difference when storing predictions.
# results = Directory to store result files in. If omitted, the current working directory is used, ***and we're assumed to just want to generate predictions if the test_location is specified.***
# log = Directory to store result files in. If omitted, the current working directory is used.
# models = Directory to store models in. Can be omitted to not store models.
# vocabs = Directory to store vocabs in. Can be omitted to not store vocabs.
# predictions = Directory to store prediction files in. Can be omitted to not store predictions.

[Input]
train_location=/home/nlp-text/dynamic/nmadnani/sentiment-analysis/features/sentiment-new/regular/expandedTrain
featuresets=['5.2-intsum', '5.2-intkbins']
classifiers=['rforest', 'svm_radial', 'svm_linear', 'logistic', 'naivebayes']
#fixed_parameters=[{}, {}, {}, {'dual': False, 'penalty': 'l2'}, {}]
suffix=.tsv

[Tuning]
grid_search=True
objective=f1_score_micro
param_grids=[[], [], [], [{'penalty': ['l1'], 'dual': [False], 'C': [1e-4, 1e-2, 1.0, 1e2, 1e4]}, {'penalty': ['l2'], 'dual': [True, False], 'C': [1e-4, 1e-2, 1.0, 1e2, 1e4]}], []]

[Output]
probability=True
results=/home/research/dblanchard/sklearn_wrapper/results
log=/home/research/dblanchard/sklearn_wrapper/log
models=/home/research/dblanchard/sklearn_wrapper/models
vocabs=/home/research/dblanchard/sklearn_wrapper/expandedTrain
predictions=/home/research/dblanchard/sklearn_wrapper/predictions
