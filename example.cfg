# Configuration file format
#
# [Input]
# train_location = Path to directory containing training data files. There must be a
#                  file for each featureset.
# test_location = Path to directory containing training data files. There must be a
#                 file for each featureset. If unspecified, cross-validation is
#                 performed.
# cv_folds_location = Path to a csv file (with a header that is ignored) specifying 
#					  folds for cross-validation.
#                     The first column should consist of training set IDs and the 
#                     second should be a string for the fold ID 
#                     (e.g., 1 through 5, A through D, etc.).  If specified, 
#                     the CV and grid search will leave one fold ID out at a time.
#                     Note: K-1 folds will be used for grid search within CV, so there
#                     should be more at least 3 fold IDs.

# featuresets = List of lists of prefixes for the files containing the features
#               you would like to train/test on.  Each list will end up being a job.
# featureset_names = Optional list of names for the feature sets.  If omitted,
#                    then the prefixes will be munged together to make names.
# classifiers = List of sklearn models to try using. Acceptable values are
#               "logistic", "svm_linear", "svm_radial", "naivebayes", "dtree",
#               "rforest", and "gradient".
#
#               logistic = Logistic regression using LibLinear
#               svm_linear = SVM using LibLinear
#               svm_radial = SVM using LibSVM
#               naivebayes = Multinomial Naive Bayes
#               dtree = Decision Tree
#               rforest = Random Forest
#               gradient = Gradient Boosting
#               ridge = Ridge Regression
#               rescaled_ridge = Ridge Regression, with predictions rescaled
#                and constrained to better match the training set.
#               svr_linear = Support Vector Regression with a linear kernel.
#               rescaled_svr_linear = Linear SVR, with predictions rescaled
#                and constrained to better match the training set.

#
# fixed_parameters = List of dicts containing parameters you want to have fixed
#                    for each classifier in list. Any empty ones will be ignored
#                    (and the defaults will be used).
# suffix = The file format the training/test files are in. Valid option are ".tsv",
#          ".megam", and ".jsonlines" (one complete JSON dict per line in the file).
#
#
# [Tuning]
# grid_search = Whether or not to perform grid search to find optimal parameters
#               for classifier.
# grid_search_jobs = Number of folds to run in parallel when using grid search.
#                    Defaults to number of grid search folds.
# objective = The objective function to use for tuning. Valid options are
#             "f1_score_micro" (micro-averaged f-score), "f1_score_macro"
#             (macro-averaged f-score), "f1_score_least_frequent" (optimize the
#             f1 score of the least frequent class), "accuracy", "spearman"
#             (Spearman rank-correlation), "pearson" (Pearson correlation),
#             "kendall_tau" (Kendall"s tau), "quadratic_weighted_kappa"
#             (The Q. W. kappa, where any floating point values are rounded),
#             and "unweighted_kappa" (unweighted Cohen's kappa, where the classes
#             should be ints).
# param_grids = List of parameter grids to search for each classifier. Each
#               parameter grid should be a list of of dictionaries mapping
#               from strings to lists of parameter values. When you specify an
#               empty list for a classifier, the default parameter grid for that
#               classifier will be searched.
# scale_features = Whether to scale features by their mean (for dense data only)
#                  and standard deviation.  This defaults to True.
# use_dense_features = Whether the features should be converted to dense matrices.
#                      This defaults to False.
#
#
# [Output]
# probability = Whether or not to output probabilities for each class instead of
#               the most probable class for each instance. Only really makes a
#               difference when storing predictions.
# results = Directory to store result files in. If omitted, the current working
#           directory is used, ***and we're assumed to just want to generate
#           predictions if the test_location is specified.***
# log = Directory to store result files in. If omitted, the current working
#       directory is used.
# models = Directory to store models in. Can be omitted to not store models.
# vocabs = Directory to store vocabs in. Can be omitted to not store vocabs.
# predictions = Directory to store prediction files in. Can be omitted to not
#               store predictions.


[Input]
train_location=/home/nlp-text/dynamic/nmadnani/sentiment-analysis/features/sentiment-new/regular/expandedTrain
featuresets=["5.2-intsum", "5.2-intkbins"]
featureset_names=["sum", "bins"]
classifiers=["rforest", "svm_radial", "svm_linear", "logistic", "naivebayes"]
#fixed_parameters=[{}, {}, {}, {"dual": false, "penalty": "l2"}, {}]
suffix=.tsv

[Tuning]
grid_search=true
objective=f1_score_micro
param_grids=[[], [], [], [{"penalty": ["l1"], "dual": [false], "C": [1e-4, 1e-2, 1.0, 1e2, 1e4]}, {"penalty": ["l2"], "dual": [true, false], "C": [1e-4, 1e-2, 1.0, 1e2, 1e4]}], []]

[Output]
probability=true
results=/home/research/dblanchard/sklearn_wrapper/results
log=/home/research/dblanchard/sklearn_wrapper/log
models=/home/research/dblanchard/sklearn_wrapper/models
vocabs=/home/research/dblanchard/sklearn_wrapper/expandedTrain
predictions=/home/research/dblanchard/sklearn_wrapper/predictions
